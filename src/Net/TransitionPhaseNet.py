import random

import numpy as np
import pytorch_lightning as pl
import torch
from torch import nn

from src.Datasets.BatchProcessor import BatchProcessDatav2
from src.Module.Utilities import PLU
from src.Net.CommonOperation import CommonOperator
# from src.Datasets.TransitionDataModule import Transition_DataModule, BatchRotateYCenterXZ
from src.geometry.quaternions import normalized_or6d
from src.geometry.quaternions import quat_to_or6D, or6d_to_quat
from src.utils.BVH_mod import Skeleton, find_secondary_axis


def eval_sample(model, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, pos_offset, skeleton: Skeleton, length, param):
    # FOR EXAMPLE
    model = model.eval()
    model = model.cuda()
    quats = Q
    offsets = pos_offset
    hip_pos = X
    dict = {"hip_pos": X, 'offsets': Q, 'quats': Q}
    gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)
    loc_rot = quat_to_or6D(gq)
    if "target_id" in param:
        target_id = param["target_id"]
    else:
        target_id = length + 10
    noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device)
    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)
    tar_quat = quat_to_or6D(tar_quat)
    target_style = model.get_film_code(tar_pos.cuda(),tar_quat.cuda())
    F = S[:, 1:] - S[:, :-1]
    F = model.phase_op.remove_F_discontiny(F)
    F = F / model.phase_op.dt
    phases = model.phase_op.phaseManifold(A, S)
    if(model.predict_phase==True):
     pred_pos, pred_rot, pred_phase, _,_ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(), F.cuda(), target_style, None,
                                                               start_id=10,
                                                               target_id=target_id, length=length,
                                                               phase_schedule=1.)
    else:
        pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),
                                                                   F.cuda(), target_style, None,
                                                                   start_id=10,
                                                                   target_id=target_id, length=length,
                                                                   phase_schedule=1.)
    pred_pos,pred_rot = pred_pos.cpu(),pred_rot.cpu()
    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])
    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]
    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)
    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)

    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))
    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :]).flatten(-2, -1)
    x_mean = x_mean.view(skeleton.num_joints, 3)
    x_std = x_std.view(skeleton.num_joints, 3)
    GX = (GX - x_mean.flatten(-2, -1)) / x_std.flatten(-2, -1)
    GX = GX.transpose(1, 2)
    return GQ, GX

from src.Module.PhaseModule import PhaseOperator
class StateEncoder(nn.Module):
    def __init__(self,inchannels):
        super(StateEncoder, self).__init__()
        self.layers = torch.nn.ModuleList([torch.nn.Linear(inchannels, 512),torch.nn.Linear(512,256)])
        self.acts = torch.nn.ModuleList([PLU(),PLU()])

    def forward(self,x):
        for i in range(len(self.layers)):
            x = self.layers[i](x)
            x = self.acts[i](x)
        return x
class StyleEmbedding(torch.nn.Module):
    def __init__(self,in_channels):
        super(StyleEmbedding, self).__init__()
        from src.Net.TransitionNet import AdaInNorm2D,ATNBlock
        self.adains = nn.ModuleList([AdaInNorm2D(512, 512, 0)])
        self.atns = nn.ModuleList([ATNBlock(512, 512)])
        self.linears = nn.ModuleList([nn.Linear(in_channels, 512)])
        self.act = nn.ELU()

    def FILM(self, idx, s, x,pos_encoding, first):
        x = self.adains[idx](s, x,pos_encoding, first)
        x = self.act(x)
        x = self.atns[idx](s, x,pos_encoding, first)
        return x


    def forward(self, fs, condition,pos_encoding, first):
        '''input: N,C->decoder'''
        '''phase: N,C->gate network'''
        '''x: pose+latent'''
        # just for sure it has style
        x = condition
        x = self.linears[0](x)
        x = self.FILM(0,fs[0],x,None,first)
        x = self.act(x)


        return x
class MoeStylePhasePredictor(nn.Module):
    def __init__(self, rnn_size, phase_dim, num_experts):
        from src.Net.TransitionNet import AdaInNorm2D,ATNBlock
        from src.Module.PhaseModule import PhaseSegement
        super(MoeStylePhasePredictor, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(rnn_size+phase_dim*2+512,rnn_size),nn.Linear(rnn_size,512),nn.Linear(512,512)])
        self.adains = nn.ModuleList([AdaInNorm2D(512,512,0)])
        self.atns = nn.ModuleList([ATNBlock(512, 512)])
        self.act = nn.ModuleList([nn.ELU(), nn.ELU(), nn.ELU()])
        self.mlp = nn.Linear(512,phase_dim*4+9+32)
        self.phase_predcitor = PhaseSegement(phase_dim)
        self.phase_dims = phase_dim


    def FILM(self, idx, s, x, pos_encoding, first):
        x = self.adains[idx](s, x, pos_encoding, first)
        x = self.act[idx](x)
        x = self.atns[idx](s, x, pos_encoding, first)
        return x

    def forward(self, fs, condition, phase,offset_latent,first):
        '''input: N,C->decoder'''
        '''phase: N,C->gate network'''
        '''x: pose+latent'''

        x = condition
        x = torch.cat((x,phase.flatten(-2,-1),offset_latent),dim=-1)
        x = self.linears[0](x)
        x = self.act[0](x)
        x = self.linears[1](x)
        x = self.FILM(0,fs[0],x,None,first)
        x = self.act[1](x)
        x = self.linears[2](x)
        x = self.act[2](x)
        out = self.mlp(x)
        phase,hip,latent = out[:,:self.phase_dims*4],out[:,self.phase_dims*4:self.phase_dims*4+9],out[:,self.phase_dims*4+9:]
        phase, A, F = self.phase_predcitor(phase)
        hip_v,hip_rv = hip[:,:3],hip[:,3:]
        return phase, A, F, hip_v,hip_rv,latent
class PredictInitialState(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_layer = nn.Sequential(
            nn.Conv1d(1024,1024,5),
           # nn.AvgPool1d(2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Conv1d(1024, 1024, 3),
          #  nn.AvgPool1d(2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Conv1d(1024, 1024, 2),
            nn.ReLU()
        )

        #self.transform = nn.Transformer(1024)
       # self.bn = lambda v:(v[:,0]**2+v[:,1]**2)
        self.mlp = nn.Linear(1024,10*3)
        self.phase_op = PhaseOperator(1 / 30.)
    def forward(self, x):
        def bn(v):
            len = torch.sqrt(v[...,0]**2+v[...,1]**2+1e-6)
            return v/len.unsqueeze(-1)
        x = x.transpose(1,2)
        x = self.conv_layer(x)
        x = x.squeeze(-1)

        # x = self.transform(x,x)
        # x = x[:,-1]
        out = self.mlp(x)
        phase = out#.view(out.shape[0],10,3)
        A,S = phase[:,:10].unsqueeze(-1),phase[:,10:30].unsqueeze(-1)
        S = S.view(S.shape[0],10,2)
        S = bn(S)
        S = torch.atan2(S[...,1],S[...,0]).unsqueeze(-1)/self.phase_op.tpi


       #  S = torch.atan2(S[:,:10],S[:,10:])/3.14159
        phase = self.phase_op.phaseManifold(A,S)
        return  phase,A,S

class TransitionNet_phase(pl.LightningModule):
    """Sequence-to-sequence model for human motion prediction"""
    def __init__(self, moe_decoder: nn.Module,skeleton, pose_channels,stat,
                  dt,  phase_dim=20, rnn_size=1024, dropout=0.3, past_seq=10,mode='pretrain',predict_phase=False,pretrained_model=None):
        from src.Net.StyleVAENet import IAN_FilmGenerator2
        from src.Net.TransitionNet import SeqScheduler,PosEncoding
        super(TransitionNet_phase, self).__init__()
        self.rot_rep_idx = [1, 5, 9, 10, 11, 12, 13, 15, 19]
        self.pos_rep_idx = [idx for idx in np.arange(0, skeleton.num_joints) if idx not in self.rot_rep_idx]
        self.mode = mode
        self.test = True
        self.predict_phase = predict_phase
        pos_mean, pos_std = stat['pos_stat']
        vel_mean, vel_std = stat['vel_stat']
        rot_mean, rot_std = stat["rot_stat"]
        register_numpy = lambda x, s: self.register_buffer(s, torch.from_numpy(x).float())
        register_scalar = lambda x, s: self.register_buffer(s, torch.Tensor([x]).float())
        register_numpy(vel_mean, "vel_mean")
        register_numpy(pos_mean, "pos_mean")
        register_scalar(vel_std, "vel_std")
        register_numpy(rot_mean, "rot_mean")
        register_scalar(rot_std, "rot_std")
        register_scalar(pos_std, "pos_std")
        self.automatic_optimization = True
        max_seq_length = 40
        self.seq_scheduler = SeqScheduler(20, 40)
        self.seq_scheduler2 = SeqScheduler(5,20)
        self.max_seq = max_seq_length
        self.past_seq = past_seq
        self.learned_embedding = False
        self.batch_processor = BatchProcessDatav2()
        self.phase_dim = phase_dim
        self.dt = dt
        #self.VAE_op = VAE_Pose_Operator(skeleton)
        self.phase_op = PhaseOperator(dt)
        self.lr = self.init_lr = 1e-3
        self.normal_method = 'zscore'
        self.input_channel = pose_channels
        self.save_hyperparameters(ignore=['moe_decoder','mode','pretrained_model'])
        self.skeleton = skeleton
        num_joints = skeleton.num_joints
        self.num_joints = num_joints
        self.state_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*6 # +phase_dim*2# c_t, root velocity, q_t
        self.offset_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*3  # offset from target, root offset, pose offset
        self.target_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*6 # + phase_dim*2# q_target
        self.dropout = dropout
        self.state_encoder = StateEncoder(self.state_input_size)
        self.offset_encoder = StateEncoder(self.offset_input_size)
        self.target_encoder = StateEncoder(self.target_input_size)
        self.embedding_style = StyleEmbedding(256)
        self.LSTMCell = torch.nn.LSTMCell(1024, rnn_size)
        self.initial_state_predictor = PredictInitialState()
        self.pose_channels = pose_channels
        self.phase_predictor = MoeStylePhasePredictor(rnn_size, phase_dim,8)
        self.film_generator = IAN_FilmGenerator2(12 * 22)
        self.decoder = moe_decoder.decoder

        self.sigma_target = 0.5
        self.max_tta = past_seq + max_seq_length - 5
        if (self.learned_embedding):
            self.embedding = nn.Embedding(self.max_tta + 1, 256)
            self.embedding512 = nn.Embedding(self.max_tta+1,512)
        else:
            self.embedding = PosEncoding(self.max_tta + 1, 256)
            self.embedding512 = PosEncoding(self.max_tta + 1, 512)
        if (pretrained_model != None):
            if(type(pretrained_model)==dict):
                self.load_state_dict(pretrained_model['state_dict'], strict=False)
            else:
                self.load_state_dict(pretrained_model.state_dict(), strict=False)

        self.l1_loss = nn.L1Loss()
        self.mse_loss = nn.MSELoss()
        self.common_operator = CommonOperator(batch_size=32)
    def target_encoding(self, target_rots,target_pos, target_v):
        return self.target_encoder(torch.cat((target_pos.flatten(-2,-1),target_v.flatten(-2,-1),target_rots.flatten(-2,-1)), dim=-1))

    def state_encoding(self, pos,vel, rot):
        return self.state_encoder(torch.cat((pos.flatten(-2,-1),vel.flatten(-2,-1), rot.flatten(-2,-1)), dim=-1))

    def offset_encoding(self, pos, rot):
        return self.offset_encoder(torch.cat((pos.flatten(-2,-1),rot.flatten(-2,-1)), dim=-1))

    def regu_pose(self, pos, edge_len, rot):
        import src.geometry.inverse_kinematics as ik
        pos = ik.scale_pos_to_bonelen(pos, edge_len, self.skeleton._level_joints, self.skeleton._level_joints_parents)
        rot = normalized_or6d(rot)
        return pos, rot


    def shift_running(self, local_pos, local_rots,phases,As,Fs, style_code, noise_per_sequence, start_id, target_id,length,phase_schedule=1.):
        from src.Net.TransitionNet import concat
        if not length:
            length = target_id - start_id

        '''pose: N,T,J,9'''
        '''hip: N,T,3'''
        '''phases: N,T,M'''
        '''style_code: [N,C,T,J| N,C,T,J]'''
        J = local_pos.shape[-2]
        N, T, J, C = local_pos.shape
        device = local_pos.device

        output_pos = torch.empty(size=(N, length,  (J),3), device=device)
        output_rot = torch.empty(size=(N, length,  (J),6), device=device)
        output_phase = torch.empty(size=(N, length, phases.shape[-2], 2), device=phases.device)
        output_sphase = torch.empty(size=(N, length, phases.shape[-2], 2), device=phases.device)
        output_A = torch.empty(size=(N, length, phases.shape[-2], 1), device=phases.device)
        output_F = torch.empty(size=(N, length, phases.shape[-2], 1), device=phases.device)
        #
        hn = torch.zeros(N, 1024, device=device)
        cn = torch.zeros(N, 1024, device=device)

        if (noise_per_sequence == None):
            noise_per_sequence = torch.normal(0, 0.5, size=(N, 512), device=device)
        offset_t = torch.scalar_tensor(start_id + length - 1, dtype=torch.int32, device=device)
        tmax = torch.scalar_tensor(self.max_tta, dtype=torch.int32, device=device)
        local_pos = local_pos[:,:,self.pos_rep_idx]
        last_g_v = local_pos[:, 1] - local_pos[:, 0]
        target_g_pos, target_g_rots = local_pos[:, target_id-1, :], local_rots[:,target_id-1, :]
        target_g_v = local_pos[:,target_id-1]-local_pos[:,target_id-2]
        last_g_pos, last_g_rot  = local_pos[:, 1, :], local_rots[:, 1, :]


        latent_loss = 0.

        target_latent = self.target_encoding(target_g_rots, target_g_pos-target_g_pos[:,0:1], target_g_v)

        encode_first = True
        if(hasattr(self,"predict_phase")==False or self.predict_phase==False):
            for i in range(1,start_id-1):
                last_l_v, target_l_v = last_g_v, target_g_v
                last_l_pos, target_l_pos = last_g_pos, target_g_pos
                last_l_rot, target_l_rot = last_g_rot, target_g_rots
                offset_pos = target_l_pos - last_l_pos
                offset_rot = target_l_rot - last_l_rot
                offset_t = offset_t - 1

                state_latent = self.state_encoding(last_l_pos-last_l_pos[:,0:1], last_l_v, last_l_rot)
                offset_latent = self.offset_encoding(offset_pos, offset_rot)
                state_latent = self.embedding_style(style_code, state_latent, None, encode_first)
                latent,h_target = concat(state_latent, offset_latent, target_latent, self.embedding,self.embedding512, noise_per_sequence, offset_t, tmax)
                encode_first = False
                (hn, cn) = self.LSTMCell(latent, (hn, cn))
                last_g_pos,last_g_rot  = local_pos[:,i+1],local_rots[:,i+1]
                last_g_v = local_pos[:,i+1]-local_pos[:,i]
                last_phase = phases[:,i+1]
        else:
            last_l_v, target_l_v = local_pos[:,1:start_id-1]-local_pos[:,0:start_id-2], target_g_v.repeat(1,start_id-2,1,1)
            last_l_pos,target_l_pos = local_pos[:,1:start_id-1],target_g_pos.unsqueeze(1).repeat(1,start_id-2,1,1)
            last_l_rot,target_l_rot = local_rots[:,1:start_id-1],target_g_rots.unsqueeze(1).repeat(1,start_id-2,1,1)
            offset_pos = target_l_pos-last_l_pos
            offset_rot = target_l_rot-last_l_rot
            last_l_pos = last_l_pos.flatten(0,1)
            last_l_v = last_l_v.flatten(0,1)
            last_l_rot = last_l_rot.flatten(0,1)
            offset_pos = offset_pos.flatten(0,1)
            offset_rot = offset_rot.flatten(0,1)

            state_latent = self.state_encoding(last_l_pos - last_l_pos[:, 0:1], last_l_v, last_l_rot)
            offset_latent = self.offset_encoding(offset_pos, offset_rot)
            style_code[0] = style_code[0].unsqueeze(1).repeat(1,start_id-2,1,1).flatten(0,1)
            state_latent = self.embedding_style(style_code, state_latent, None, first=True)

            target_latent = target_latent.unsqueeze(1).repeat(1,start_id-2,1).flatten(0,1)
            ## recover
            recover = lambda x: x.view((N,start_id-2)+x.shape[1:])
            state_latent = recover(state_latent)
            offset_latent = recover(offset_latent)
            target_latent = recover(target_latent)
            style_code[0] = recover(style_code[0])

            offset_ts = []
            for i in range(1,start_id-1):
                offset_t = offset_t - 1
                offset_ts.append(offset_t.view(1))
            offset_ts = torch.cat(offset_ts,dim=0)

            from src.Net.TransitionNet import multi_concat
            latent, h_target = multi_concat(state_latent, offset_latent, target_latent, self.embedding, self.embedding512,noise_per_sequence,offset_ts, tmax)
            pre_phase, preA, preS = self.initial_state_predictor(latent)

            style_code[0] = style_code[0][:,0]
            target_latent = target_latent[:,0]

            # prepare for predicting the first frame
            last_g_pos,last_g_rot  = local_pos[:,start_id-1],local_rots[:,start_id-1]
            last_g_v = local_pos[:,start_id-1]-local_pos[:,start_id-2]
            last_phase = pre_phase
           # last_phase = phases[:,start_id-1]
            for i in range(1,start_id-1):
                (hn, cn) = self.LSTMCell(latent[:,i-1], (hn, cn))

        first = True
        step = 0
        for i in range(start_id-1, start_id-1+length):
            last_l_v, target_l_v = last_g_v, target_g_v
            last_l_pos, target_l_pos = last_g_pos, target_g_pos
            last_l_rot, target_l_rot = last_g_rot, target_g_rots
            offset_pos = target_l_pos - last_l_pos
            offset_rot = target_l_rot - last_l_rot
            offset_t = offset_t - 1

            state_latent = self.state_encoding(last_l_pos - last_l_pos[:, 0:1], last_l_v, last_l_rot)
            offset_latent = self.offset_encoding(offset_pos, offset_rot)
            state_latent = self.embedding_style(style_code, state_latent, None, encode_first)
            encode_first=False
            latent,h_target = concat(state_latent, offset_latent, target_latent, self.embedding,self.embedding512, noise_per_sequence, offset_t,tmax)
            (hn, cn) = self.LSTMCell(latent, (hn, cn))

            input_clip = hn
            pred_phase,pred_A,pred_F,hip_l_v,hip_l_rv,latent = self.phase_predictor(style_code,input_clip,last_phase,h_target,first)
            hip_l_r = hip_l_rv + last_l_rot[:,0]
            condition_no_style = torch.cat(((last_l_pos - last_l_pos[:, 0:1]).flatten(-2,-1), last_l_v.flatten(-2,-1), hip_l_v, last_l_rot.flatten(-2,-1), hip_l_r), dim=-1)
            nxt_phase = self.phase_op.next_phase(last_phase, pred_A, pred_F)
            slerp_phase = self.phase_op.slerp(nxt_phase, pred_phase)
            pred_pose_, coefficients = self.decoder(latent, condition_no_style,slerp_phase)
            pred_l_v, pred_l_rot_v = pred_pose_[..., :len(self.pos_rep_idx) * 3], pred_pose_[..., len(self.pos_rep_idx) * 3:]
            pred_l_v = pred_l_v.view(-1,len(self.pos_rep_idx),3)
            pred_l_rot_v = pred_l_rot_v.view(-1, self.skeleton.num_joints, 6)

            pred_g_v = pred_l_v
            pred_g_v[:,0] = hip_l_v
            pred_rot = pred_l_rot_v+last_l_rot
            pred_rot[:,0] = hip_l_r
            pred_pos = pred_g_v + last_g_pos

            output_pos[:,step, self.pos_rep_idx] = pred_pos
            output_rot[:, step] = pred_rot
            output_phase[:,step] = pred_phase
            output_A[:,step] = pred_A
            output_F[:,step] = pred_F
            output_sphase[:,step] = slerp_phase

            last_g_pos, last_g_rot = pred_pos, pred_rot
            last_g_v = pred_g_v
            last_phase = slerp_phase

            first = False
            step+=1

        latent_loss = latent_loss/length
        #output = output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss#,output_hip_pos,output_hip_rot#, output_phase,output_A,output_F
        if(hasattr(self,"predict_phase") and self.predict_phase):
            return output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss,[pre_phase,preA,preS]
        else:
            return output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss


    def phase_loss(self, gt_phase, gt_A, gt_F, phase, A, F, sphase):
        loss = {"phase": self.mse_loss(gt_phase, phase), "A": self.mse_loss(gt_A, A), "F": self.mse_loss(gt_F, F),
                "slerp_phase": self.mse_loss(gt_phase, sphase)}
        return loss
    def weight_sum_loss(self, loss, weight):
        l = 0
        for key in loss.keys():
            l = l + loss[key] * weight[key] if key in weight.keys() else l + loss[key]
        return l
    def rot_to_pos(self,rot,offsets,hip):
        if (rot.shape[-1] == 6):
            rot = or6d_to_quat(rot)
        rot_pos = self.skeleton.global_rot_to_global_pos(rot, offsets, hip)
        return rot_pos

    def get_gt_contact(self,gt_v):
        eps = 0.5
        eps_bound = 1.
        def contact_smooth(x, idx, eps, bound):
            # x must be in range of [eps,bound]
            x = (x - eps) / (bound - eps)
            x2 = x * x
            x3 = x2 * x
            contact = 2 * (x3) - 3 * (x2) + 1
            return contact * idx
        key_joint = [3, 4, 7, 8]
        gt_fv = gt_v[:, :, key_joint]
        gt_fv = torch.sum(gt_fv.pow(2), dim=-1).sqrt()
        # first we need gt_contact
        idx = (gt_fv < eps)  # * (pred_fv>gt_fv) # get penality idx
        idx_smooth = (gt_fv >= eps) * (gt_fv < eps_bound)
        gt_contact = contact_smooth(gt_fv, idx_smooth, eps, eps_bound) + torch.ones_like(gt_fv) * idx
        return gt_contact
    def contact_foot_loss(self,contact,pred_v):
        key_joints = [3, 4, 7, 8]
        pred_fv = pred_v[:, :, key_joints]
        pred_fv = (torch.sum(pred_fv.pow(2), dim=-1) + 1e-6)  # .sqrt()
        contact_idx = torch.where(contact < 0.01, 0, contact)
        # torch.where(contact>=0.01,-contact,0) # only those who is 100% fixed, we don't apply position constrain
        contact_num = torch.count_nonzero(contact_idx)
        pred_fv = pred_fv * contact_idx
        contact_loss = pred_fv.sum() / max(contact_num, 1)
        return contact_loss
    def shared_forward(self, batch, seq, optimizer=None, d_optimizer=None):
        N = batch['local_pos'].shape[0] #// 2
        style_code = self.get_film_code(batch['sty_pos'][:N], batch['sty_rot'][:N])
        A = batch['A']
        S = batch['S']
        F = S[:, 1:] - S[:, :-1]
        F = self.phase_op.remove_F_discontiny(F)
        F = F / self.phase_op.dt
        local_pos, local_rots, edge_len, phases = self.transform_batch_to_VAE(batch)
        # source
        local_pos = local_pos[:N]
        local_rots = local_rots[:N]
        edge_len = edge_len[:N]
        phases = phases[:N]
        A = A[:N]
        F = F[:N]
        offsets = batch['offsets'][:N]
        noise=None
        start_id = 10
        target_id = 10+seq

        output = self.shift_running(local_pos, local_rots, phases, A, F,style_code, noise,
                                                                                start_id=start_id, target_id=target_id,length=seq,
                                                                                phase_schedule=self.schedule_phase)
        if(self.predict_phase):
            pred_pos, pred_rot, pred_phase, latent_loss, first_phase = output
        else:
            pred_pos, pred_rot, pred_phase, latent_loss = output
        rot_pos = self.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])
        pred_pos[:, :, self.rot_rep_idx] = rot_pos[:, :, self.rot_rep_idx]

        if (self.test == True or random.random() < 0.1):
            pred_pos, pred_rot = self.regu_pose(pred_pos, edge_len, pred_rot)
        edge_mean = 21.

        glb_loss = self.l1_loss(local_pos[:, start_id:target_id, :]/edge_mean, pred_pos[:, :, :]/edge_mean)
        glb_rot_loss = self.l1_loss(local_rots[:, start_id:target_id], pred_rot)
        last_pos_loss = self.l1_loss(local_pos[:,target_id-1,:]/edge_mean,pred_pos[:,-1]/edge_mean)
        last_rot_loss = self.l1_loss(local_rots[:,target_id-1,:],pred_rot[:,-1])
        gt_contact = self.get_gt_contact(local_pos[:,start_id+1:target_id]-local_pos[:,start_id:target_id-1])
        contact_loss = self.contact_foot_loss(gt_contact,pred_pos[:,1:]-pred_pos[:,:-1])

        phase_loss = self.phase_loss(phases[:, start_id:target_id], A[:, start_id:target_id], F[:, start_id-1:target_id-1], pred_phase[0][:, :],pred_phase[1][:, :], pred_phase[2][:, :], pred_phase[3])
        loss = {"glb_pos": glb_loss, "glb_rot": glb_rot_loss, "lst_pos": last_pos_loss, "lst_rot": last_rot_loss,
                **phase_loss, "fv": contact_loss}
        if(self.predict_phase):
            pre_phase,pre_A,pre_S = first_phase
            first_phase_loss = {"f_ph": self.mse_loss(phases[:,start_id-1], pre_phase), "f_A": self.mse_loss(A[:,start_id-1], pre_A), "f_S": self.mse_loss(S[:,start_id-1], pre_S)}
            loss = {**loss,**first_phase_loss}


        new_weight = {"glb_pos":1.,"glb_rot":1., "fv":0.01,"lst_pos":1.0,"lst_rot":0.5,"phase": 0.5, "A": 0.5, "F": 0.5, "slerp_phase": 0.5,"f_ph":0.5,"f_A":0.5,"f_S":0.5}
        loss['loss'] = self.weight_sum_loss(loss, new_weight)

        return loss, pred_pos, pred_rot


    def un_normalized(self, batch):
        batch['glb_vel'] = batch["glb_vel"] * self.vel_std + self.vel_mean
        batch['glb_rot'] = batch['glb_rot'] * self.rot_std+self.rot_mean
        batch['glb_pos'] = batch['glb_pos'] * self.pos_std+self.pos_mean
        return batch


    def normalized(self, batch):
        batch['glb_rot'] = (batch['glb_rot']-self.rot_mean)/self.rot_std
        batch['glb_vel'] = (batch['glb_vel']-self.vel_mean)/self.vel_std
        batch['glb_pos'] = (batch['glb_pos']-self.pos_mean)/self.pos_std
        return batch

    def transform_batch_to_filmEncoder(self,glb_pos,glb_rot):
        glb_vel, glb_pos, glb_rot, root_rotation = self.batch_processor.forward(glb_rot, glb_pos)
        glb_rot = quat_to_or6D(glb_rot)
        batch = {'glb_vel': glb_vel, 'glb_rot': glb_rot, 'glb_pos': glb_pos}

        batch = self.normalized(batch)
        batch = {key: batch[key][:, :, 1:] for key in batch.keys()}
        return self.transform_batch_to_input(batch)
    def transform_batch_to_input(self,batch):
        glb_vel, glb_rot, glb_pos = batch['glb_vel'],batch['glb_rot'], batch['glb_pos']
        data = torch.cat((glb_pos[:,1:],glb_vel,glb_rot[:,1:]),dim=-1)
        data  = data.permute(0,3,1,2).contiguous() # N,C,T,J
        return data
    def get_film_code(self,glb_pos,glb_rot):
        if (glb_rot.shape[-1] == 6):
            glb_rot = or6d_to_quat(glb_rot)
        data = self.transform_batch_to_filmEncoder(glb_pos, glb_rot)
        data = data.transpose(-2, -1).contiguous().flatten(1, 2)
        if self.test ==False:
            idx = torch.randperm(data.shape[0])
        else:
            idx = torch.arange(data.shape[0])
        return self.film_generator(data[idx])#,idx

    @staticmethod
    def transform_batch_to_VAE(batch):
        local_pos, local_rot = batch['local_pos'], batch['local_rot']
        edge_len = torch.norm(batch['offsets'][:, 1:], dim=-1, keepdim=True)
        return local_pos, quat_to_or6D(local_rot), edge_len,batch['phase']

    def validation_step(self, batch, batch_idx):
        self.scheduled_prob = 1.
        self.schedule_phase = 1.
        self.style_phase = 1.
        self.test =True
        loss,pred_pos,pred_rot = self.shared_forward(batch, self.seq_scheduler.max_seq)
        self.common_operator.log_dict(self, loss, "val_")
        return loss

    def test_step(self, batch, batch_idx):
        self.test=True
        self.scheduled_prob = 1.
        self.schedule_phase = 1.
        self.style_phase = 1.
        loss, pred_pos, pred_rot = self.shared_forward(batch, self.seq_scheduler.max_seq)
        self.common_operator.log_dict(self, loss, "test_")
        return loss

    def training_step(self, batch, batch_idx):
        self.test =False
        if(self.mode=='pretrain'):
            progress = self.common_operator.get_progress(self, target_epoch=3,start_epoch=0)
            self.schedule_phase = self.common_operator.get_progress(self,target_epoch=4,start_epoch=1)
            self.style_phase = self.common_operator.get_progress(self,target_epoch=6,start_epoch=3)
        else:
            progress = 1
            self.schedule_phase = 1.
        length = self.seq_scheduler.range(progress)
        '''calculate loss'''
        loss,pred_pos,pred_rot = self.shared_forward(batch, length)
        self.common_operator.log_dict(self, loss, "train_")
        self.log("length", length, logger=True)
        return loss['loss']

    def configure_optimizers(self):
        models = self.common_operator.collect_models(self)
        trained_model = []
        for param in self.parameters():
            param.requires_grad = False
        def weight_decay(model_name,lr, weight_decay):
            trained_model.append(model_name)
            model = getattr(self,model_name)
            for param in model.parameters():
                param.requires_grad = True
            return self.common_operator.add_weight_decay(model,lr, weight_decay)

        lr = self.lr
        if(self.mode=='pretrain' and self.predict_phase==False):
            params = weight_decay("film_generator",lr,1e-4)+weight_decay("target_encoder", lr, 0) + weight_decay("embedding_style",lr,0)+\
                     weight_decay("offset_encoder", lr, 0) +\
                     weight_decay("state_encoder", lr, 0) + \
                     weight_decay("LSTMCell", lr, 0) + weight_decay("phase_predictor",lr,0)#+weight_decay("decoder",lr,0)
        elif(self.predict_phase == True):
            params = weight_decay("initial_state_predictor",lr,1e-4)
        elif(self.mode=='fine_tune'):
            lr = self.lr*0.1
            params = weight_decay("film_generator", lr, 1e-4) + weight_decay("target_encoder", lr, 0) + weight_decay(
                "embedding_style", lr, 0) + \
                     weight_decay("offset_encoder", lr, 0) + \
                     weight_decay("state_encoder", lr, 0) + \
                     weight_decay("LSTMCell", lr, 0) + weight_decay("phase_predictor", lr, 0)


        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.5, 0.9), amsgrad=True)

        non_train_model=[i for i in models if i not in trained_model]
        if(len(non_train_model)>0):
            import warnings
            warnings.warn("warning:there are some models not trained:{}".format(non_train_model))

        return [optimizer]


from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule
class Application_phase(nn.Module):
    def __init__(self, net: TransitionNet_phase, data_module: StyleVAE_DataModule):
        super(Application_phase, self).__init__()
        self.Net = net
        self.data_module = data_module
        self.data_loader = data_module.loader
        self.src_batch = None
        self.target_batch = None
        self.skeleton = data_module.skeleton

    def transform_batch(self,motion,label):
        batch = [[],label]
        batch[0] = [[motion[0],motion[1],motion[2]]]
        for i, value in enumerate(batch[0][0]):
            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)
        phase = {}
        for key in motion[3].keys():
            phase[key] = torch.from_numpy(motion[3][key]).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)
        batch[0][0].append(phase)
        batch = self.data_module.transfer_mannual(batch,0,use_phase=True,use_sty=False)
        return batch

    def transform_anim(self, anim, label):
        batch = [[], label]
        batch[0] = [[anim.quats, anim.offsets, anim.hip_pos]]
        for i, value in enumerate(batch[0][0]):
            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)
        batch = self.data_module.transfer_mannual(batch, 0,use_phase=False)

        # batch = normalized(self.Net,batch,True)
        return batch

    def setSource(self, anim):
        if(type(anim)==tuple):
            self.src_batch = self.transform_batch(anim,0)
        else:
            self.src_batch = self.transform_anim(anim, 0)
        self.offsets = self.src_batch['offsets']
        self.tangent = find_secondary_axis(self.offsets)

    def setTarget(self, anim):
        if (type(anim) == tuple):
            self.target_anim = self.transform_batch(anim, 2)
        else:
            self.target_anim = self.transform_anim(anim, 2)

    def _get_transform_ori_motion(self, batch):
        global_pos = batch['local_pos']
        global_rot = batch['local_rot']
        global_rot = self.skeleton.inverse_pos_to_rot(global_rot, global_pos, self.offsets, self.tangent)
        lp, lq = self.skeleton.inverse_kinematics(global_rot, global_pos)
        return lp[0].cpu().numpy(), lq[0].cpu().numpy()

    def get_source(self):
        return self._get_transform_ori_motion(self.src_batch)
    def get_target(self):
        return self._get_transform_ori_motion(self.target_anim)

    def forward(self,t,x):
        self.Net.schedule_phase = 1.
        self.Net.style_phase = 1.
        seq = self.Net.seq_scheduler.max_seq
        # seq = 10
        self.Net.eval()
        with torch.no_grad():
            loc_pos, loc_rot, edge_len,phases = self.Net.transform_batch_to_VAE(self.src_batch)
            tar_pos,tar_rot ,_,_= self.Net.transform_batch_to_VAE(self.target_anim)
            target_style = self.Net.get_film_code(tar_pos,tar_rot)

            A = self.src_batch['A']
            S = self.src_batch['S']
            F = S[:,1:]-S[:,:-1]
            F = self.Net.phase_op.remove_F_discontiny(F)
            F = F/self.Net.phase_op.dt
            if x !=1 :
                loc_pos[:, 12:, :, [0, 2]] = loc_pos[:, 12:, :, [0, 2]] + (
                            loc_pos[:, 10 + seq, 0, [0, 2]] - loc_pos[:, 10, 0, [0, 2]]) * x
            if(self.Net.predict_phase):
                pred_pos, pred_rot, pred_phase, _,_ = self.Net.shift_running(loc_pos, loc_rot, phases, A, F,
                                                                          target_style, None, start_id=10,
                                                                          target_id=10 + seq, length=int(seq * t),
                                                                          phase_schedule=1.)
            else:
                pred_pos, pred_rot,pred_phase,_= self.Net.shift_running(loc_pos, loc_rot, phases,A,F,target_style, None, start_id=10,
                                                            target_id=10 + seq,length = int(seq*t) ,phase_schedule=1.)

            rot_pos = self.Net.rot_to_pos(pred_rot,self.src_batch['offsets'],pred_pos[:,:,0:1])
            pred_pos[:,:,self.Net.rot_rep_idx] = rot_pos[:,:,self.Net.rot_rep_idx]
            output_pos, output_rot = self.Net.regu_pose(pred_pos, edge_len, pred_rot)

            output_pos = torch.cat((loc_pos[:, :10, ...], output_pos, loc_pos[:, 10 + seq:, ...]), dim=1)
            output_rot = torch.cat((loc_rot[:, :10, ...], output_rot, loc_rot[:, 10 + seq:, ...]), dim=1)
            batch = {}
            batch['local_rot'] = or6d_to_quat(output_rot)
            batch['local_pos'] = output_pos
            return self._get_transform_ori_motion(batch)
            # output = self.src

